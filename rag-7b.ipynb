{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12558321,"sourceType":"datasetVersion","datasetId":7929846},{"sourceId":12558416,"sourceType":"datasetVersion","datasetId":7929903}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install langchain-community\n!pip install faiss-cpu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T06:59:33.118493Z","iopub.execute_input":"2025-07-25T06:59:33.119069Z","iopub.status.idle":"2025-07-25T06:59:44.461701Z","shell.execute_reply.started":"2025-07-25T06:59:33.119039Z","shell.execute_reply":"2025-07-25T06:59:44.460932Z"}},"outputs":[{"name":"stdout","text":"Collecting langchain-community\n  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\nRequirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.66)\nRequirement already satisfied: langchain<1.0.0,>=0.3.26 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.26)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.41)\nRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.4)\nRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.12.13)\nRequirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (8.5.0)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\nCollecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\nRequirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.1)\nCollecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\nRequirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (1.26.4)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\nRequirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (0.3.8)\nRequirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (2.11.7)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (1.33)\nCollecting packaging<25,>=23.2 (from langchain-core<1.0.0,>=0.3.66->langchain-community)\n  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\nRequirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (4.14.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.28.1)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (3.10.18)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\nRequirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.23.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain-community) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain-community) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain-community) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain-community) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain-community) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain-community) (2.4.1)\nCollecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.6.15)\nRequirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.3)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (4.9.0)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.0.9)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (0.16.0)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain-community) (3.0.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (2.33.2)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.26.2->langchain-community) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.26.2->langchain-community) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.26.2->langchain-community) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.26.2->langchain-community) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.26.2->langchain-community) (2024.2.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.3.1)\nDownloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\nDownloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading packaging-24.2-py3-none-any.whl (65 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\nInstalling collected packages: python-dotenv, packaging, httpx-sse, pydantic-settings, langchain-community\n  Attempting uninstall: packaging\n    Found existing installation: packaging 25.0\n    Uninstalling packaging-25.0:\n      Successfully uninstalled packaging-25.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.5.1 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\npandas-gbq 0.29.1 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nthinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\ndataproc-spark-connect 0.7.5 requires google-api-core>=2.19, but you have google-api-core 1.34.1 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed httpx-sse-0.4.1 langchain-community-0.3.27 packaging-24.2 pydantic-settings-2.10.1 python-dotenv-1.1.1\nCollecting faiss-cpu\n  Downloading faiss_cpu-1.11.0.post1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.0 kB)\nRequirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (1.26.4)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3.0,>=1.25.0->faiss-cpu) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\nDownloading faiss_cpu-1.11.0.post1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: faiss-cpu\nSuccessfully installed faiss-cpu-1.11.0.post1\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import os\nimport json\nimport pandas as pd\nimport torch\nfrom tqdm import tqdm\nfrom typing import List, Dict\nfrom kaggle_secrets import UserSecretsClient\nfrom huggingface_hub import login\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_core.documents import Document\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"hf_hub_token\")\nlogin(hf_token)\n\nCSV_FILE = \"/kaggle/input/quran-translation/Muhammad_Tahir-ul-Qadri_translation.csv\"\nQUESTIONS_FILE = \"/kaggle/input/test-jsonl/test.jsonl\"\nOUTPUT_FILE = \"/kaggle/working/rag_mistral_output.jsonl\"\nINDEX_PATH = \"/kaggle/working/quran_faiss_index\"\n\nK = 4\nMAX_TOKENS = 512\nCHUNK_SIZE = 1764\nCHUNK_OVERLAP = 0\nLIMIT = 100\nMODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.1\"\nEMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-25T06:59:44.463199Z","iopub.execute_input":"2025-07-25T06:59:44.463443Z","iopub.status.idle":"2025-07-25T07:00:05.116749Z","shell.execute_reply.started":"2025-07-25T06:59:44.463419Z","shell.execute_reply":"2025-07-25T07:00:05.116214Z"}},"outputs":[{"name":"stderr","text":"2025-07-25 06:59:51.929795: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753426792.130266      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753426792.191038      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"def load_documents_from_csv(csv_path: str) -> List[Document]:\n    df = pd.read_csv(csv_path)\n    docs = []\n    for _, row in df.iterrows():\n        content = str(row[\"Ayat\"]).strip()\n        meta = {\n            \"surah\": row[\"Surah\"],\n            \"ayah\": row[\"Ayat\"],\n            \"verse\": row[\"Verse\"]\n        }\n        if content:\n            docs.append(Document(page_content=content, metadata=meta))\n    return docs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T07:00:05.117327Z","iopub.execute_input":"2025-07-25T07:00:05.117669Z","iopub.status.idle":"2025-07-25T07:00:05.122574Z","shell.execute_reply.started":"2025-07-25T07:00:05.117652Z","shell.execute_reply":"2025-07-25T07:00:05.121860Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def build_or_load_faiss(docs: List[Document], embedder, force_recreate=False):\n    if not force_recreate and os.path.exists(INDEX_PATH):\n        return FAISS.load_local(INDEX_PATH, embedder,allow_dangerous_deserialization=True)\n\n    splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n    chunks = splitter.split_documents(docs)\n    db = FAISS.from_documents(chunks, embedder)\n    db.save_local(INDEX_PATH)\n    return db\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T07:00:05.124502Z","iopub.execute_input":"2025-07-25T07:00:05.124982Z","iopub.status.idle":"2025-07-25T07:00:05.398396Z","shell.execute_reply.started":"2025-07-25T07:00:05.124946Z","shell.execute_reply":"2025-07-25T07:00:05.397625Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def load_questions(path: str, limit: int = 100) -> List[Dict[str, str]]:\n    questions = []\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            if len(questions) >= limit:\n                break\n            try:\n                record = json.loads(line)\n                question = record.get(\"Question\") or record.get(\"question\")\n                if question and isinstance(question, str):\n                    questions.append({\"Question\": question.strip()})\n            except json.JSONDecodeError:\n                continue\n    return questions\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T07:00:05.399121Z","iopub.execute_input":"2025-07-25T07:00:05.399377Z","iopub.status.idle":"2025-07-25T07:00:05.412765Z","shell.execute_reply.started":"2025-07-25T07:00:05.399351Z","shell.execute_reply":"2025-07-25T07:00:05.412097Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def save_results(path: str, results: List[Dict]):\n    with open(path, \"w\", encoding=\"utf-8\") as f:\n        for record in results:\n            f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n\ndef generate_answer(pipe, question: str, context_chunks: List[Document]):\n    context_str = \"\\n\".join([doc.page_content for doc in context_chunks])\n    prompt = f\"\"\"\n<|user|>\nContext:\n{context_str}\n\nQuestion: {question}\n<|end|>\n\"\"\"\n    result = pipe(prompt, max_new_tokens=MAX_TOKENS, return_full_text=False, temperature=0.1)\n    return result[0]['generated_text'].strip()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T07:00:05.413492Z","iopub.execute_input":"2025-07-25T07:00:05.413659Z","iopub.status.idle":"2025-07-25T07:00:05.432299Z","shell.execute_reply.started":"2025-07-25T07:00:05.413645Z","shell.execute_reply":"2025-07-25T07:00:05.431734Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def main():\n    print(\"🔁 Loading documents...\")\n    embedder = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)\n    documents = load_documents_from_csv(CSV_FILE)\n\n    print(\"🔁 Building/loading FAISS index...\")\n    vector_db = build_or_load_faiss(documents, embedder)\n    retriever = vector_db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": K})\n\n    print(\"🧠 Loading Mistral-7B model...\")\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n    model = AutoModelForCausalLM.from_pretrained(\n        MODEL_NAME,\n        torch_dtype=torch.bfloat16,\n        attn_implementation=\"sdpa\",\n        device_map=\"auto\"\n    )\n    pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n\n    print(\"📖 Loading questions...\")\n    questions = load_questions(QUESTIONS_FILE)\n\n    results = []\n    for idx, q in enumerate(tqdm(questions, desc=\"Running RAG\")):\n        question = q.get(\"Question\", \"\").strip()\n        if not question:\n            continue\n\n        retrieved_docs = retriever.get_relevant_documents(question)\n        answer = generate_answer(pipe, question, retrieved_docs)\n\n        reference = [\n            f\"Surah {doc.metadata.get('surah')}\"\n            for doc in retrieved_docs\n        ]\n        \n        retrieved_texts = [f\"{doc.metadata.get('verse')}\" for doc in retrieved_docs]\n\n        results.append({\n            \"Question\": question,\n            \"Answer\": answer,\n            \"Retrieved\": retrieved_texts,\n            \"Reference\": reference\n        })\n\n    print(\"💾 Saving results...\")\n    save_results(OUTPUT_FILE, results)\n    print(f\"✅ Done! Output saved to: {OUTPUT_FILE}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T07:00:05.432983Z","iopub.execute_input":"2025-07-25T07:00:05.433270Z","iopub.status.idle":"2025-07-25T07:00:05.450754Z","shell.execute_reply.started":"2025-07-25T07:00:05.433238Z","shell.execute_reply":"2025-07-25T07:00:05.450075Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def check():\n    print(\"🔁 Loading documents...\")\n    embedder = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)\n    documents = load_documents_from_csv(CSV_FILE)\n\n    print(\"🔁 Building/loading FAISS index...\")\n    vector_db = build_or_load_faiss(documents, embedder)\n    retriever = vector_db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": K})\n\n    print(\"🧠 Loading Mistral-7B model...\")\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n    model = AutoModelForCausalLM.from_pretrained(\n        MODEL_NAME,\n        torch_dtype=torch.bfloat16,\n        attn_implementation=\"sdpa\",\n        device_map=\"auto\"\n    )\n\n    pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n\n    question = \"Fasting in severe injury or medical emergency , ruling for muslims in ramadan month\"\n\n    retrieved_docs = retriever.get_relevant_documents(question)\n    answer = generate_answer(pipe, question, retrieved_docs)\n    reference = [\n            f\"Surah {doc.metadata.get('surah')}\"\n            for doc in retrieved_docs\n        ]\n    \n    retrieved_texts = [f\"{doc.metadata.get('verse')}\" for doc in retrieved_docs]\n    print(f\"Answer: {answer}\")\n    print(f\"\\nRetrieved Doc: {retrieved_docs}\")\n    print(f\"\\nReference: {reference}\")\n    print(f\"\\nRetrieved Text: {retrieved_texts}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T07:00:05.451394Z","iopub.execute_input":"2025-07-25T07:00:05.451617Z","iopub.status.idle":"2025-07-25T07:00:05.470014Z","shell.execute_reply.started":"2025-07-25T07:00:05.451573Z","shell.execute_reply":"2025-07-25T07:00:05.469481Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# check()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T07:00:05.470654Z","iopub.execute_input":"2025-07-25T07:00:05.470877Z","iopub.status.idle":"2025-07-25T07:00:05.486556Z","shell.execute_reply.started":"2025-07-25T07:00:05.470862Z","shell.execute_reply":"2025-07-25T07:00:05.485846Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# import shutil\n# shutil.rmtree(\"/kaggle/working/quran_faiss_index/\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T07:00:05.488144Z","iopub.execute_input":"2025-07-25T07:00:05.488322Z","iopub.status.idle":"2025-07-25T07:00:05.500731Z","shell.execute_reply.started":"2025-07-25T07:00:05.488308Z","shell.execute_reply":"2025-07-25T07:00:05.500167Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T07:00:05.501395Z","iopub.execute_input":"2025-07-25T07:00:05.502013Z","iopub.status.idle":"2025-07-25T07:18:18.321750Z","shell.execute_reply.started":"2025-07-25T07:00:05.501993Z","shell.execute_reply":"2025-07-25T07:18:18.321103Z"}},"outputs":[{"name":"stdout","text":"🔁 Loading documents...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/623437731.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n  embedder = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b94c926e5d1845bcb942e8ec548a485d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"499c703cb1b64594993fca8006090ae4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa076eb5989b474085824de5a2b05a35"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"540e318978bb44f4975bec29bb6cb115"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19beb79e405f42c796ae32f5346ac140"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95754f8a3fab4ce68bbf82d2a35eef8b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9b9eaf67b47405a979e23f9fb92b403"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb0c2ebdf3c04bc286f9bf7a5ca5cd39"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31bf33a7ff574b5d8b23877f51ebc568"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69992b1b7ba0416ca9a7540b55f0f1e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7d5067db8964707a42494eb282eccb4"}},"metadata":{}},{"name":"stdout","text":"🔁 Building/loading FAISS index...\n🧠 Loading Mistral-7B model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.10k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ac2d718b04a43edbb61d2977b005c33"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72388ae2d9594033bfff38df627d1bdd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7883f666b8045cc81221c8ecb936179"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ac4022282b54615a96667e2f2397974"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca72ba1693f44ecfb8f07e5ee9b3e86d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79c459c81c3049758e7a9b7198564032"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"167a0bcacc72445e96d277cdc8066639"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d9140eb78e3424f9df6d7ecaf0ab3f1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43b2f2e3e070453b8454bad12f3dc180"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bbe035ac9a28472780ede67cd2edd057"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cedf353aaecb40b39d84d5bbdd21af06"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"📖 Loading questions...\n","output_type":"stream"},{"name":"stderr","text":"Running RAG:   0%|          | 0/100 [00:00<?, ?it/s]/tmp/ipykernel_36/623437731.py:29: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n  retrieved_docs = retriever.get_relevant_documents(question)\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:   1%|          | 1/100 [00:11<19:43, 11.95s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:   2%|▏         | 2/100 [00:15<11:24,  6.99s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:   3%|▎         | 3/100 [00:34<19:58, 12.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:   4%|▍         | 4/100 [00:40<16:08, 10.09s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:   5%|▌         | 5/100 [00:51<16:02, 10.13s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:   6%|▌         | 6/100 [01:04<17:29, 11.17s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:   7%|▋         | 7/100 [01:13<16:36, 10.71s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:   8%|▊         | 8/100 [01:23<15:39, 10.22s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:   9%|▉         | 9/100 [01:36<16:46, 11.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  10%|█         | 10/100 [01:38<12:45,  8.50s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  11%|█         | 11/100 [01:49<13:30,  9.10s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  12%|█▏        | 12/100 [01:54<11:32,  7.86s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  13%|█▎        | 13/100 [01:58<09:48,  6.77s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  14%|█▍        | 14/100 [02:16<14:29, 10.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  15%|█▌        | 15/100 [02:20<11:41,  8.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  16%|█▌        | 16/100 [02:44<18:23, 13.13s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  17%|█▋        | 17/100 [02:50<15:10, 10.97s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  18%|█▊        | 18/100 [02:57<13:10,  9.64s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  19%|█▉        | 19/100 [03:04<12:01,  8.91s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  20%|██        | 20/100 [03:09<10:30,  7.88s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  21%|██        | 21/100 [03:10<07:30,  5.70s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  22%|██▏       | 22/100 [03:17<07:58,  6.13s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  23%|██▎       | 23/100 [03:26<08:44,  6.81s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  24%|██▍       | 24/100 [03:31<08:07,  6.41s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  25%|██▌       | 25/100 [03:44<10:23,  8.32s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  26%|██▌       | 26/100 [03:55<11:14,  9.12s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  27%|██▋       | 27/100 [04:08<12:29, 10.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  28%|██▊       | 28/100 [04:20<12:56, 10.78s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  29%|██▉       | 29/100 [04:33<13:43, 11.60s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  30%|███       | 30/100 [04:47<14:15, 12.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  31%|███       | 31/100 [04:51<11:13,  9.76s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  32%|███▏      | 32/100 [04:56<09:27,  8.34s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  33%|███▎      | 33/100 [04:57<06:43,  6.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  34%|███▍      | 34/100 [05:00<05:45,  5.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  35%|███▌      | 35/100 [05:01<04:07,  3.81s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  36%|███▌      | 36/100 [05:09<05:38,  5.30s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  37%|███▋      | 37/100 [05:11<04:19,  4.13s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  38%|███▊      | 38/100 [05:16<04:38,  4.49s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  39%|███▉      | 39/100 [05:23<05:24,  5.32s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  40%|████      | 40/100 [05:24<03:52,  3.87s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  41%|████      | 41/100 [05:40<07:34,  7.70s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  42%|████▏     | 42/100 [05:53<08:44,  9.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  43%|████▎     | 43/100 [06:13<11:57, 12.60s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  44%|████▍     | 44/100 [06:17<09:20, 10.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  45%|████▌     | 45/100 [06:30<09:51, 10.75s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  46%|████▌     | 46/100 [06:37<08:42,  9.68s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  47%|████▋     | 47/100 [06:53<10:13, 11.57s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  48%|████▊     | 48/100 [06:58<08:10,  9.43s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  49%|████▉     | 49/100 [07:10<08:52, 10.43s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  50%|█████     | 50/100 [07:22<09:00, 10.82s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  51%|█████     | 51/100 [07:32<08:44, 10.69s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  52%|█████▏    | 52/100 [07:33<06:08,  7.68s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  53%|█████▎    | 53/100 [07:45<07:02,  8.98s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  54%|█████▍    | 54/100 [07:47<05:18,  6.93s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  55%|█████▌    | 55/100 [07:48<03:46,  5.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  56%|█████▌    | 56/100 [07:54<04:00,  5.47s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  57%|█████▋    | 57/100 [07:58<03:30,  4.90s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  58%|█████▊    | 58/100 [08:03<03:25,  4.88s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  59%|█████▉    | 59/100 [08:15<04:46,  7.00s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  60%|██████    | 60/100 [08:24<05:07,  7.70s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  61%|██████    | 61/100 [08:40<06:40, 10.27s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  62%|██████▏   | 62/100 [09:07<09:33, 15.10s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  63%|██████▎   | 63/100 [09:11<07:16, 11.79s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  64%|██████▍   | 64/100 [09:18<06:12, 10.34s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  65%|██████▌   | 65/100 [09:20<04:39,  7.98s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  66%|██████▌   | 66/100 [09:35<05:43, 10.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  67%|██████▋   | 67/100 [09:50<06:21, 11.55s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  68%|██████▊   | 68/100 [09:54<04:58,  9.32s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  69%|██████▉   | 69/100 [10:09<05:42, 11.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  70%|███████   | 70/100 [10:14<04:32,  9.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  71%|███████   | 71/100 [10:26<04:50, 10.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  72%|███████▏  | 72/100 [10:32<04:07,  8.85s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  73%|███████▎  | 73/100 [10:42<04:04,  9.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  74%|███████▍  | 74/100 [10:42<02:49,  6.52s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  75%|███████▌  | 75/100 [10:54<03:19,  7.96s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  76%|███████▌  | 76/100 [10:58<02:45,  6.92s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  77%|███████▋  | 77/100 [11:07<02:50,  7.40s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  78%|███████▊  | 78/100 [11:20<03:25,  9.32s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  79%|███████▉  | 79/100 [11:32<03:30, 10.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  80%|████████  | 80/100 [11:43<03:25, 10.25s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  81%|████████  | 81/100 [11:46<02:35,  8.21s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  82%|████████▏ | 82/100 [12:03<03:13, 10.74s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  83%|████████▎ | 83/100 [12:20<03:34, 12.62s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  84%|████████▍ | 84/100 [12:26<02:51, 10.72s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  85%|████████▌ | 85/100 [12:40<02:52, 11.52s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  86%|████████▌ | 86/100 [12:46<02:18,  9.92s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  87%|████████▋ | 87/100 [12:54<02:03,  9.46s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  88%|████████▊ | 88/100 [12:58<01:31,  7.66s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  89%|████████▉ | 89/100 [13:05<01:22,  7.53s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  90%|█████████ | 90/100 [13:25<01:53, 11.38s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  91%|█████████ | 91/100 [13:38<01:45, 11.72s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  92%|█████████▏| 92/100 [13:41<01:13,  9.14s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  93%|█████████▎| 93/100 [13:51<01:05,  9.30s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  94%|█████████▍| 94/100 [14:04<01:02, 10.44s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  95%|█████████▌| 95/100 [14:10<00:46,  9.20s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  96%|█████████▌| 96/100 [14:17<00:33,  8.41s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  97%|█████████▋| 97/100 [14:43<00:41, 13.94s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  98%|█████████▊| 98/100 [14:48<00:22, 11.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG:  99%|█████████▉| 99/100 [14:50<00:08,  8.55s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nRunning RAG: 100%|██████████| 100/100 [15:13<00:00,  9.14s/it]","output_type":"stream"},{"name":"stdout","text":"💾 Saving results...\n✅ Done! Output saved to: /kaggle/working/rag_mistral_output.jsonl\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":15}]}